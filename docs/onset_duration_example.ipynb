{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Using onset/duration format\n",
    "\n",
    "Many datasets store interval annotations as onset + duration rather than explicit interval boundaries. This notebook shows how to use `DimensionInterval` with onset/duration coordinates directly, without needing to construct `pd.IntervalIndex` objects.\n",
    "\n",
    "## Why onset/duration?\n",
    "\n",
    "Common data formats like TextGrid, Praat, and many annotation tools export intervals as:\n",
    "- `onset`: when the interval starts\n",
    "- `duration`: how long the interval lasts\n",
    "\n",
    "The `linked_indices` library provides helper functions to convert annotation DataFrames directly to xarray coordinates with proper naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from linked_indices import DimensionInterval, example_data\n",
    "from linked_indices.example_data import (\n",
    "    intervals_from_dataframe,\n",
    "    intervals_from_long_dataframe,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Loading annotation data\n",
    "\n",
    "Annotation data typically comes as a pandas DataFrame with onset, duration, and label columns. Let's load some example speech annotation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example speech annotations (simulating data from Praat, TextGrid, etc.)\n",
    "annotations = example_data.speech_annotations()\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Notice that the annotations have **gaps** between them - this is common in real speech data where there are pauses between words. For example, \"hello\" ends at 1.7s but \"world\" doesn't start until 2.1s.\n",
    "\n",
    "## Converting DataFrame to xarray coordinates\n",
    "\n",
    "The `intervals_from_dataframe` function converts annotation DataFrames to xarray Datasets with properly named coordinates (`{dim}_onset`, `{dim}_duration`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert annotations DataFrame to xarray coordinates\n",
    "word_coords = intervals_from_dataframe(annotations, dim_name=\"word\", label_col=\"word\")\n",
    "word_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "The helper automatically creates:\n",
    "- `word` as the dimension coordinate (from `label_col`)\n",
    "- `word_onset` and `word_duration` as coordinates (named `{dim}_onset`, `{dim}_duration`)\n",
    "\n",
    "## Adding audio data\n",
    "\n",
    "Now we can add our continuous audio signal and merge with the annotation coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simulated audio signal\n",
    "times, audio_signal = example_data.generate_audio_signal(duration=10.0)\n",
    "\n",
    "# Create dataset by merging annotation coordinates with audio data\n",
    "ds = word_coords.copy()\n",
    "ds[\"audio\"] = ((\"time\",), audio_signal)\n",
    "ds = ds.assign_coords(time=times)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Applying the DimensionInterval index\n",
    "\n",
    "To link the time and word dimensions, apply `DimensionInterval` with the `onset_duration_coords` option mapping dimension names to `(onset_coord, duration_coord)` tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.drop_indexes([\"time\", \"word\"]).set_xindex(\n",
    "    [\"time\", \"word_onset\", \"word_duration\", \"word\"],\n",
    "    DimensionInterval,\n",
    "    onset_duration_coords={\"word\": (\"word_onset\", \"word_duration\")},\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.xindexes[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coord_viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coord_inspector[\"word\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Notice that:\n",
    "- The `word_onset` and `word_duration` coordinates remain visible\n",
    "- All coordinates are linked under a single `DimensionInterval` index\n",
    "- No manual coordinate creation was needed - the helper handled naming conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Selecting data\n",
    "\n",
    "Selection works exactly the same as with the IntervalIndex format. When you select on any dimension, all other dimensions are automatically constrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select by word label - time is automatically constrained\n",
    "ds.sel(word=\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select by time range - words are automatically constrained\n",
    "ds.sel(time=slice(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select by onset value\n",
    "ds.sel(word_onset=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Handling gaps\n",
    "\n",
    "Our word annotations have gaps between them (silence between words). Let's see what happens when we select time in a gap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time 1.8 to 2.0 is in the gap between \"hello\" (ends at 1.7) and \"world\" (starts at 2.1)\n",
    "ds.sel(time=slice(1.75, 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "When selecting multiple words with gaps between them using `isel`, the time dimension spans the **union** of their intervals (including the gap). Here we select \"hello\" [0.5, 1.7) and \"world\" [2.1, 3.9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first two words - time spans from 0.5 to 3.9, including the gap\n",
    "ds.isel(word=slice(0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Multiple onset/duration dimensions\n",
    "\n",
    "You can have multiple interval dimensions, each with their own onset/duration coordinates. This is common for hierarchical annotations like words and phonemes. The helper function makes it easy to convert each level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multi-level annotations (words and phonemes)\n",
    "word_annotations, phoneme_annotations = example_data.multi_level_annotations()\n",
    "\n",
    "display(word_annotations)\n",
    "display(phoneme_annotations)\n",
    "\n",
    "# Convert each DataFrame to xarray coordinates using helpers\n",
    "word_ds = intervals_from_dataframe(word_annotations, dim_name=\"word\", label_col=\"word\")\n",
    "phoneme_ds = intervals_from_dataframe(\n",
    "    phoneme_annotations, dim_name=\"phoneme\", label_col=\"phoneme\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge annotation coordinates and add audio data\n",
    "times, audio = example_data.generate_audio_signal(duration=10.0)\n",
    "\n",
    "ds_multi = xr.merge([word_ds, phoneme_ds])\n",
    "ds_multi[\"audio\"] = ((\"time\",), audio)\n",
    "ds_multi = ds_multi.assign_coords(time=times)\n",
    "\n",
    "# Apply index with both onset/duration mappings\n",
    "ds_multi = ds_multi.drop_indexes([\"time\", \"word\", \"phoneme\"]).set_xindex(\n",
    "    [\n",
    "        \"time\",\n",
    "        \"word_onset\",\n",
    "        \"word_duration\",\n",
    "        \"word\",\n",
    "        \"part_of_speech\",\n",
    "        \"phoneme_onset\",\n",
    "        \"phoneme_duration\",\n",
    "        \"phoneme\",\n",
    "    ],\n",
    "    DimensionInterval,\n",
    "    onset_duration_coords={\n",
    "        \"word\": (\"word_onset\", \"word_duration\"),\n",
    "        \"phoneme\": (\"phoneme_onset\", \"phoneme_duration\"),\n",
    "    },\n",
    ")\n",
    "ds_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select word \"hello\" - both time AND phonemes are constrained\n",
    "ds_multi.sel(word=\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select by part of speech - finds all nouns\n",
    "ds_multi.sel(part_of_speech=\"noun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Controlling interval closedness\n",
    "\n",
    "By default, intervals are left-closed `[onset, onset+duration)`. You can change this with the `interval_closed` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data for fresh example\n",
    "annotations = example_data.speech_annotations()\n",
    "times, audio = example_data.generate_audio_signal()\n",
    "\n",
    "# Create with right-closed intervals (onset, onset+duration]\n",
    "ds_right = xr.Dataset(\n",
    "    {\"audio\": ((\"time\",), audio)},\n",
    "    coords={\n",
    "        \"time\": times,\n",
    "        \"word_onset\": (\"word\", annotations[\"onset\"].values),\n",
    "        \"word_duration\": (\"word\", annotations[\"duration\"].values),\n",
    "        \"word\": (\"word\", annotations[\"word\"].values),\n",
    "    },\n",
    ")\n",
    "\n",
    "ds_right = ds_right.drop_indexes([\"time\", \"word\"]).set_xindex(\n",
    "    [\"time\", \"word_onset\", \"word_duration\", \"word\"],\n",
    "    DimensionInterval,\n",
    "    onset_duration_coords={\"word\": (\"word_onset\", \"word_duration\")},\n",
    "    interval_closed=\"right\",  # Options: \"left\", \"right\", \"both\", \"neither\"\n",
    ")\n",
    "print(\"Created dataset with right-closed intervals (onset, onset+duration]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The onset/duration format provides a convenient way to work with interval data without manually constructing `pd.IntervalIndex` objects:\n",
    "\n",
    "1. **Load annotations** as a pandas DataFrame (from TextGrid, Praat, CSV, etc.)\n",
    "2. **Convert to coordinates** using `intervals_from_dataframe()` or `intervals_from_long_dataframe()`\n",
    "3. **Merge and add data** - combine annotation coordinates with your continuous data\n",
    "4. **Apply the index** with `onset_duration_coords` mapping\n",
    "5. **Select data** - all selection operations work identically to IntervalIndex format\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "| Function | Use case |\n",
    "|----------|----------|\n",
    "| `intervals_from_dataframe()` | Convert a single-event-type DataFrame |\n",
    "| `intervals_from_long_dataframe()` | Convert a multi-event-type DataFrame with category column |\n",
    "\n",
    "### Key features\n",
    "\n",
    "- **Natural representation**: Use onset + duration directly from your data files\n",
    "- **Library helpers**: Handle coordinate naming conventions automatically\n",
    "- **Visible coordinates**: onset and duration remain as regular coordinates  \n",
    "- **Full functionality**: All selection operations work identically\n",
    "- **Multiple dimensions**: Support for multiple onset/duration pairs\n",
    "- **Gap support**: Non-contiguous intervals work correctly\n",
    "- **Mixed events**: Handle DataFrames with multiple event types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Handling multiple event types in one DataFrame\n",
    "\n",
    "Sometimes annotation data comes as a single \"long format\" DataFrame with multiple event types (words, phonemes, stimuli, etc.) distinguished by a category column. The `intervals_from_long_dataframe` function handles this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example mixed-event annotations\n",
    "mixed_df = example_data.mixed_event_annotations()\n",
    "mixed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all event types at once\n",
    "intervals_from_long_dataframe(mixed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time/audio and apply DimensionInterval\n",
    "times, audio = example_data.generate_audio_signal(duration=10.0)\n",
    "interval_ds = intervals_from_long_dataframe(mixed_df)\n",
    "\n",
    "ds_mixed = interval_ds.copy()\n",
    "ds_mixed[\"audio\"] = ((\"time\",), audio)\n",
    "ds_mixed = ds_mixed.assign_coords(time=times)\n",
    "\n",
    "# Apply the index with all three event types\n",
    "ds_mixed = ds_mixed.drop_indexes([\"time\", \"word\", \"phoneme\", \"stimulus\"]).set_xindex(\n",
    "    [\n",
    "        \"time\",\n",
    "        \"word_onset\",\n",
    "        \"word_duration\",\n",
    "        \"word\",\n",
    "        \"phoneme_onset\",\n",
    "        \"phoneme_duration\",\n",
    "        \"phoneme\",\n",
    "        \"stimulus_onset\",\n",
    "        \"stimulus_duration\",\n",
    "        \"stimulus\",\n",
    "    ],\n",
    "    DimensionInterval,\n",
    "    onset_duration_coords={\n",
    "        \"word\": (\"word_onset\", \"word_duration\"),\n",
    "        \"phoneme\": (\"phoneme_onset\", \"phoneme_duration\"),\n",
    "        \"stimulus\": (\"stimulus_onset\", \"stimulus_duration\"),\n",
    "    },\n",
    ")\n",
    "ds_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a stimulus constrains words and phonemes too\n",
    "ds_mixed.sel(stimulus=\"image_A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Manual iteration for selective event types\n",
    "\n",
    "If you only want some event types, you can filter and apply `intervals_from_dataframe` iteratively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include words and phonemes (exclude stimuli)\n",
    "datasets = []\n",
    "for event_type in [\"word\", \"phoneme\"]:\n",
    "    subset = mixed_df[mixed_df[\"event_type\"] == event_type].drop(columns=[\"event_type\"])\n",
    "    ds_subset = intervals_from_dataframe(subset, dim_name=event_type, label_col=\"label\")\n",
    "    datasets.append(ds_subset)\n",
    "\n",
    "xr.merge(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
